{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#functions or classes from user defined files\n",
    "import Data_pipline\n",
    "import metrics\n",
    "import models\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_seed_globally()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class training_CAE:\n",
    "    def __init__(self,model,dataset,lr = 0.0001,optimizer=tf.keras.optimizers.Adam,epochs=100,batch_size=32,\n",
    "                 random_vectors_for_decoder=None,save_dir=os.path.join(\"..\"),loss_type=\"mse\"):\n",
    "        self.model = model\n",
    "        self.lr = lr \n",
    "        self.optimizer = optimizer(self.lr)\n",
    "        self.epochs = epochs\n",
    "        self.dataset = dataset\n",
    "        self.dataset_size = self.dataset.train_dataset_size\n",
    "        self.batch_size = batch_size\n",
    "        self.train_data = self.dataset.train_data.cache()\n",
    "        self.random_vectors_for_decoder = random_vectors_for_decoder\n",
    "        self.save_dir = save_dir\n",
    "        self.loss_batch_rec = []\n",
    "        self.loss_type = loss_type\n",
    "        self.loss_type_list = [\"mse\",\"bce\"]\n",
    "        if self.loss_type not in self.loss_type_list:\n",
    "            raise ValueError(\"Loss type not defined use amonr these\",self.loss_type_list)\n",
    "        self.metric_loss_dic = {\"epoch\":[],\"reconstruction_loss\":[]}\n",
    "    \n",
    "    def Reconstruction_loss_mse(self,decoded_x,x):\n",
    "        loss = tf.reduce_mean(tf.math.reduce_sum(tf.keras.losses.MSE(decoded_x,x),axis=[1,2]))\n",
    "        return loss\n",
    "    \n",
    "    def Reconstruction_loss(self,decoded_x,x):\n",
    "        loss = tf.reduce_mean(tf.math.reduce_sum(tf.keras.losses.binary_crossentropy(x,decoded_x),axis=[1,2]))\n",
    "        return loss\n",
    "     \n",
    "    def metrics_calc_logging(self,epoch,reconstruction_loss,epoch_end=False):\n",
    "        if epoch_end:\n",
    "            self.metric_loss_dic[\"reconstruction_loss\"].append(np.mean(self.loss_batch_rec))\n",
    "            self.metric_loss_dic[\"epoch\"].append(epoch)\n",
    "            self.loss_batch = []\n",
    "            print(\"\\n========> Reconstruction Loss : %.4f\"%(self.metric_loss_dic[\"reconstruction_loss\"][epoch]))\n",
    "            \n",
    "            if self.random_vectors_for_decoder: \n",
    "                z = self.model.encoder(np.array(self.random_vectors_for_decoder[\"images\"]),training=False)\n",
    "                decoded_x = self.model.decoder(z,training=False)\n",
    "                fig, rows = plt.subplots(nrows = self.dataset.number_of_classes, ncols=2, figsize=(10,40))\n",
    "                fig.subplots_adjust(hspace = .1, wspace=.005)\n",
    "                row = rows.ravel()\n",
    "                for i in range(0,self.dataset.number_of_classes):\n",
    "                    row[i*2].imshow(self.random_vectors_for_decoder[\"images\"][i],cmap=\"gray\")\n",
    "                    row[i*2].axis(\"off\")\n",
    "                    row[i*2].set_title(\"class_\"+str(self.random_vectors_for_decoder[\"labels\"][i]))\n",
    "                    row[i*2+1].imshow(decoded_x[i],cmap='gray')\n",
    "                    row[i*2+1].axis(\"off\")\n",
    "                    row[i*2+1].set_title(\"class_\"+str(self.random_vectors_for_decoder[\"labels\"][i]))\n",
    "                plt.savefig(os.path.join(self.save_dir,\"Decoded images per epoch\",\"decoded_imgs_epoch\"+str(epoch)+\".jpg\"))\n",
    "    #             plt.show()\n",
    "            plt.close()\n",
    "            return\n",
    "        self.loss_batch_rec.append(reconstruction_loss.numpy())\n",
    "            \n",
    "           \n",
    "            \n",
    "    @tf.function   \n",
    "    def forward_backward_prop(self,x,x_o,training=True):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z = self.model.encoder(x,training=training)\n",
    "            decoded_x = self.model.decoder(z,training=training)\n",
    "            reconstruction_loss = self.Reconstruction_loss(decoded_x,x_o)\n",
    "            total_loss = reconstruction_loss\n",
    "        if training:\n",
    "            grads = tape.gradient(total_loss, self.model.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "        return reconstruction_loss\n",
    "    \n",
    "    @tf.function   \n",
    "    def forward_backward_prop_mse(self,x,x_o,training=True):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z = self.model.encoder(x,training=training)\n",
    "            decoded_x = self.model.decoder(z,training=training)\n",
    "            reconstruction_loss = self.Reconstruction_loss_mse(decoded_x,x_o)\n",
    "            total_loss = reconstruction_loss\n",
    "        if training:\n",
    "            grads = tape.gradient(total_loss, self.model.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "        return reconstruction_loss\n",
    "    \n",
    "    def __call__(self):\n",
    "        name = str(input(\"Enter the extra name for experiment folder if wanted else enter No :\"))\n",
    "        if name!=\"No\":\n",
    "            save_dir = os.path.join(self.save_dir,name)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.mkdir(save_dir)\n",
    "            self.save_dir = save_dir\n",
    "        if self.random_vectors_for_decoder:\n",
    "            img_dir = os.path.join(self.save_dir,\"Decoded images per epoch\")\n",
    "            if not os.path.exists(img_dir):\n",
    "                os.mkdir(img_dir)\n",
    "        total_Train_batches = int(np.ceil(self.dataset_size/self.batch_size))\n",
    "        prev_acc = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            start_time = time.time()\n",
    "            train_data = self.train_data\n",
    "            print(\"\\nEpoch {} / {}\".format(epoch+1,self.epochs))\n",
    "            train_data = train_data.shuffle(self.dataset_size)\n",
    "            train_data = train_data.batch(self.batch_size)\n",
    "            train_data = train_data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "            for batch,(img,img_o,label) in train_data.enumerate(1):\n",
    "                print(\"====>Training Batch {} / {}\".format(batch,total_Train_batches),end=\"\\r\")\n",
    "                if self.loss_type==\"mse\":\n",
    "                    reconstruction_loss= self.forward_backward_prop_mse(img,img_o,training=True)\n",
    "                else:\n",
    "                    reconstruction_loss= self.forward_backward_prop(img,img_o,training=True)\n",
    "                self.metrics_calc_logging(epoch,reconstruction_loss)\n",
    "            self.metrics_calc_logging(epoch,None,epoch_end=True)\n",
    "                            \n",
    "            end_time = time.time()\n",
    "            print(\"========>Time taken : \",end_time-start_time)\n",
    "        print(\"====>Model weights of last epoch are saved in\",os.path.join(self.save_dir,\"Final_CAE.h5\"))\n",
    "        print(\"Saving the train log in  : \",self.save_dir,\"============>\")\n",
    "        df_train = pd.DataFrame.from_dict(self.metric_loss_dic)\n",
    "        df_train.to_csv(os.path.join(self.save_dir,\"train_log.csv\"),index=False)\n",
    "        self.model.save_weights(os.path.join(self.save_dir,\"Fianl_CAE.h5\"))\n",
    "        return self.model          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in Training dataset :  60000\n",
      "Images are normalized in the range [0,1] \n",
      "Belonging to the  10 Classes\n",
      "Both images and class labels are present at the output in the Train Dataset\\Test Dataset : (image,image,label)\n",
      "Time taken to load the data  :  14.027065753936768  Seconds =====================>\n",
      "Enter the extra name for experiment folder if wanted else enter No :No\n",
      "\n",
      "Epoch 1 / 10\n",
      "====>Training Batch 938 / 938\n",
      "========> Reconstruction Loss : 91.1937\n",
      "========>Time taken :  71.53777551651001\n",
      "\n",
      "Epoch 2 / 10\n",
      "====>Training Batch 938 / 938\n",
      "========> Reconstruction Loss : 65.7444\n",
      "========>Time taken :  38.90909266471863\n",
      "\n",
      "Epoch 3 / 10\n",
      "====>Training Batch 938 / 938\n",
      "========> Reconstruction Loss : 51.3032\n",
      "========>Time taken :  39.002678632736206\n",
      "\n",
      "Epoch 4 / 10\n",
      "====>Training Batch 938 / 938\n",
      "========> Reconstruction Loss : 43.5796\n",
      "========>Time taken :  38.695595264434814\n",
      "\n",
      "Epoch 5 / 10\n",
      "====>Training Batch 938 / 938\n",
      "========> Reconstruction Loss : 38.7111\n",
      "========>Time taken :  38.72041392326355\n",
      "\n",
      "Epoch 6 / 10\n",
      "====>Training Batch 938 / 938\n",
      "========> Reconstruction Loss : 35.3054\n",
      "========>Time taken :  38.956812381744385\n",
      "\n",
      "Epoch 7 / 10\n",
      "====>Training Batch 938 / 938\n",
      "========> Reconstruction Loss : 32.7504\n",
      "========>Time taken :  38.77316093444824\n",
      "\n",
      "Epoch 8 / 10\n",
      "====>Training Batch 938 / 938\n",
      "========> Reconstruction Loss : 30.7476\n",
      "========>Time taken :  39.35370373725891\n",
      "\n",
      "Epoch 9 / 10\n",
      "====>Training Batch 938 / 938\n",
      "========> Reconstruction Loss : 29.1324\n",
      "========>Time taken :  38.99570822715759\n",
      "\n",
      "Epoch 10 / 10\n",
      "====>Training Batch 938 / 938\n",
      "========> Reconstruction Loss : 27.7975\n",
      "========>Time taken :  38.958258867263794\n",
      "====>Model weights of last epoch are save in ..\\Final_CAE.h5\n",
      "Saving the train log in  :  .. ============>\n"
     ]
    }
   ],
   "source": [
    "# Data_Pipeline is implemented in the tesnoflow 2 using tf.data API this Data_Pipeline class returns the tf.dataset object\n",
    "# there are lots of functionalites about this class you can also load the dataset which is in the local folders in the format\n",
    "# Dataset_name/Class_name/imgs given its folder path using this Data_pipline class \n",
    "dataset = Data_pipline.Data_Pipeline(dataset_path=None,dataset=\"mnist\",image_size = (28,28),image_preprocessing=\"1\",\n",
    "                                     split=False,split_ratio=[0.8,0.2],labels_required_for_output=True,\n",
    "                                     images_required_for_output=True,)\n",
    "\n",
    "#Choosing the images for the observation of Decoder output \n",
    "sample_of_the_dataset = {\"images\":[],\"labels\":[]}\n",
    "for idx,(img,img_o,label) in dataset.train_data.enumerate().as_numpy_iterator():\n",
    "    if len(sample_of_the_dataset[\"labels\"])!=dataset.number_of_classes:\n",
    "        if label not in sample_of_the_dataset[\"labels\"]:\n",
    "            sample_of_the_dataset[\"labels\"].append(label)\n",
    "            sample_of_the_dataset[\"images\"].append(img)\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        break\n",
    "\n",
    "#The models present in the models.py are written only for mnist and fashion mnist dataset.\n",
    "model = models.CAE_MNIST_VGG(embedding_size=8)\n",
    "\n",
    "training = training_CAE(model = model,\n",
    "                        dataset = dataset,\n",
    "                        epochs = 10,\n",
    "                        batch_size = 64,\n",
    "                        random_vectors_for_decoder = sample_of_the_dataset,\n",
    "                        optimizer = tf.keras.optimizers.Adam,\n",
    "                        save_dir = os.path.join(\"..\"),\n",
    "                        loss_type = \"mse\",\n",
    "                        lr = 0.0001\n",
    "\n",
    "                        )\n",
    "trained_model = training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
